---
title: "对特征值和特征向量的理解"
date: 2019-06-02
categories:
  - 统计方法
  - 数学基础
tags:
  - 统计
  - 线性代数
slug: eigen_vector
---

线性空间`$V$`到自身的映射通常称为`$V$`的一个变换，如果这个变换能够保持向量的加法和数乘运算（加法和数乘运算后的结果仍然在该空间内），我们称之为**线性变换**。

比如在二维平面空间中，我们要把某个点绕着原点顺时针旋转`$\theta$`度角，等价于把坐标轴绕着原点逆时针旋转`$\theta$`度角，从直觉上来理解，所有的点仍然在这个二维空间中，只是坐标系变了，那么相应的每个点的坐标（向量的值）会发生改变。

在新的坐标系下，我们可以根据该点之前的坐标`$(x,y)$`和坐标轴旋转的角度`$\theta$`来计算它在新的坐标系下的坐标`$(x^\prime,y^\prime)$`。利用三角函数计算可得：
`$$
\begin{bmatrix}{x^\prime}\\{y^\prime}\end{bmatrix}=
\begin{bmatrix}{x \cos \theta+y \sin \theta}\\{y \cos \theta-x \sin \theta}\end{bmatrix}=
\begin{bmatrix}{\cos \theta}&{\sin \theta}\\{-\sin \theta}&{\cos \theta}\end{bmatrix}
\begin{bmatrix}{x}\\{y}\end{bmatrix}
$$`

我们发现，该线性变换后的坐标可以用一个矩阵来左乘原坐标。根据线性变换的严格定义，可以证明在每一组基下一个线性变换都可以对应一个矩阵，称为该变换在该基下的矩阵，进一步还可以证明新的坐标可以用该变换的矩阵左乘旧坐标得到。对于我们这个逆时针旋转坐标轴的问题，其实就可以转化成矩阵乘法的问题。很多时候，线性变换并不只是简单地旋转坐标轴，有可能还会改变尺度，但本质上是相同的。

设`$\mathscr{A}$`是线性空间`$V$`的一个线性变换，如果对于任意数`$\lambda_0$`都存在一个非零向量`$\xi$`，使得`$\mathscr{A}\xi=\lambda_0\xi$`，则称`$\lambda_0$`为线性变换`$\mathscr{A}$`的一个特征值（Eigen Value），`$\xi$`为特征值`$\lambda_0$`的一个特征向量（Eigen Vector）。我们知道一个线性变换`$\mathscr{A}$`可以对应一个矩阵`$A$`，我们也用这种方式来定义矩阵`$A$`的特征值和特征向量，满足`$A\xi=\lambda_0\xi$`即可。

从几何意义来说，对向量进行线性变换可以理解成改变坐标轴，或者是坐标轴不变，让点运动起来。而向量的数乘表示对向量的长度进行缩放（或者变为反向），不改变轴的方向。所以对一个线性变换来说，如果某个变量经过变换之后其方向不变，那么就是特征向量。而如果`$\theta$`是一个锐角，进行旋转后，每个点的方向都变了，所以不存在特征向量。但如果`$\theta$`为`$180^{\circ}$`的整数倍，那么就存在特征向量了。

可以证明，若`$A$`是`$k\times k$`单位阵，则满足多项式方程`$|A-\lambda I|=0$`的所有标量`$\lambda_1$`,`$\lambda_2$`,`$\cdots$`,`$\lambda_k$`都为矩阵`$A$`的特征值。如果我们把`$A$`的第`$i$`个特征值记为`$\lambda_i$`，第i个特征向量记为`$e_i$`，则有`$A=\sum^k_{i=1}\lambda_ie_ie^{\prime}_{i}$`。这种表示矩阵的方式称为**特征分解**（Eigen Decomposition），又称为谱分解（Spectral Decomposition），在很多领域中都发挥着重要的作用。
