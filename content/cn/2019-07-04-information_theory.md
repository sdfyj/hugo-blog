---
title: "香农的信息论"
date: 2019-07-04
categories:
  - 统计学方法
  - 计算机编程
tags:
  - 信息论
slug: information_theory
---

信息的作用是如何客观、定量地体现出来的呢？信息用途的背后是否有理论基础呢？对于这两个问题，几千年来都没有人给出很好的解答。
直到1948年，香农（Claude Shannon）在他著名的论文”通信的数学原理“（A Mathematic Theory of Communication）中提出
了”信息熵“的概念，才解决了信息的度量问题，并且量化出信息的作用。

信息论是一门用数理统计方法来研究信息的度量、传递和变换规律的科学。它主要是研究通讯和控制系统中普遍存在着信息传递的共同规律以及研究最佳解决信息的获限、度量、变换、储存和传递等问题的基础理论。

信息熵(Entropy)
`$$H(X)=-\sum_{x\in X}P(x)\log P(x)$$`

条件熵
`$$H(X|Y)=-\sum_{x\in X,y\in Y}P(x,y)\log P(x|y)$$`

互信息
`$$I(X;Y)=\sum_{x\in X,y\in Y}P(x,y)\log \frac{P(x,y)}{P(x)P(y)}$$`
`$$I(X;Y)=H(X)-H(X|Y)$$`
所谓两个事件相关性的量化度量，就是在了解了其中一个`$Y$`的前提下，对消除另一个`$X$`不确定性所提供的信息量。

